---
title: "Artificial Neural Network (ANN)"
author: "Clifford Mwenda"
date: "2024-04-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


DATA PREPROCESSING

Import Data Set
```{r}
setwd("C:/Users/Latitude/Desktop/Mushroom Classification")
dataset = read.csv("mushroom.csv", na.strings = c("", " ", "NA"))
str(dataset)
```

Make Character Variables Factors
```{r}
library(dplyr)
dataset <- dataset %>%
  mutate(across(-c(cap.diameter, stem.height, stem.width), as.factor))
str(dataset)
```
Check DV Distribution
```{r}
prop.table(table(dataset$class))

#Here we want to confirm that the distribution between the two label data is not too much different. Because imbalanced data-sets can lead to imbalanced accuracy.
```

Check NAs and Drop Columns with NAs
```{r}
n_nas = colSums(is.na(dataset))
(n_nas/61069) * 100
dataset = dataset %>% select(where(~ !any(is.na(.))))
str(dataset)
```

Re-Arrange Columns
```{r}
dataset = dataset[, c(2:12, 1)]
str(dataset)
```

Convert Categorical Features to Numeric for ANN
```{r}
dataset$cap.shape = as.numeric(as.integer(dataset$cap.shape))
dataset$cap.color = as.numeric(as.integer(dataset$cap.color))
dataset$does.bruise.or.bleed = as.numeric(as.integer(dataset$does.bruise.or.bleed))
dataset$gill.color = as.numeric(as.integer(dataset$gill.color))
dataset$stem.color = as.numeric(as.integer(dataset$stem.color))
dataset$has.ring = as.numeric(as.integer(dataset$has.ring))
dataset$habitat = as.numeric(as.integer(dataset$habitat))
dataset$season = as.numeric(as.integer(dataset$season))

str(dataset)
```

Split Data-set into Training and Testing
```{r}
library("caTools")
set.seed(123)
split = sample.split(dataset$class, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

Feature Scaling
```{r}
training_set[-12] = scale(training_set[-12]) #Z Score
test_set[-12] = scale(test_set[-12])


str(training_set)
```







FIITING AND EVALUATING THE MODEL

Fitting Artificial Neural Network to the Training set, Predicting the Test set results, and Making the Confusion Matrix
```{r}
#Fitting ANN to the Training Set
# install.packages("h2o")
library(h2o)
#Establish connection to an online h2 instance
#You can connect to a server but we will use a default one.We need a server because we have high computation
#We need more cores. That is why we prefer GPUs over CPUs
h2o.init(nthreads = -1)#-1 specifies all available cores available (optimize)
#I had to convert response variable to factor because the model advised on the same
classifier = h2o.deeplearning(y = "class",
                              training_frame = as.h2o(training_set),
                              activation = "Rectifier",
                              hidden = c(50, 50),#Number of neurons in first and second hidden layers
                              epochs = 500,#Number of iterations
                              train_samples_per_iteration = -2)#Auto-tuned Batch Size


#Predicting the Test set results
prob_pred = h2o.predict(classifier, 
                        newdata = as.h2o(test_set[-12]))
y_pred = ifelse(prob_pred > 0.5, 1, 0) #We get the predictions in form of Boolean
#Convert the environment to a vector
y_pred = as.vector(y_pred[3])

cm = table(test_set[, 12], y_pred)
cm
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[2, 2] + cm[1, 2] + cm[2, 1])
accuracy 
#When hidden = c(6, 6) and epochs = 100, accuracy was 69.60%
#When hidden = c(10, 10) and epochs = 100, accuracy was 76.72%
#When hidden = c(20, 20) and epochs = 100, accuracy was 88.64%
#When hidden = c(50, 50) and epochs = 500, accuracy was 93.07%
```

Apply k-Fold Cross-Validation 
```{r}
library(caret)
library(e1071)
# in creating the folds we specify the target feature (dependent variable) and # of folds
folds = createFolds(training_set$class, k = 10)
# in cv we are going to applying a created function to our 'folds'
cv = lapply(folds, function(x) { # start of function
  # in the next two lines we will separate the Training set into it's 10 pieces
  training_fold = training_set[-x, ] # training fold =  training set minus (-) it's sub test fold
  test_fold = training_set[x, ] # here we describe the test fold individually
  # now apply (train) the classifer on the training_fold
  classifier = h2o.deeplearning(y = "class",
                              training_frame = as.h2o(training_fold),
                              activation = "Rectifier",
                              hidden = c(50, 50),#Number of neurons in first and second hidden layers
                              epochs = 500,#Number of iterations
                              train_samples_per_iteration = -2)#Auto-tuned Batch Size
  # next step in the loop, we calculate the predictions and cm and we equate the accuracy
  # note we are training on training_fold and testing its accuracy on the test_fold
  prob_pred = h2o.predict(classifier, 
                        newdata = as.h2o(test_fold[-12]))
  y_pred = ifelse(prob_pred > 0.5, 1, 0) #We get the predictions in form of Boolean
#Convert the environment to a vector
  y_pred = as.vector(y_pred[3])
  cm = table(test_fold[, 12], y_pred)
  return(cm)
})


# Initialize a vector to store accuracies for each fold
accuracies <- numeric(length = length(cv))
# Iterate over each confusion matrix and calculate accuracy
for (i in seq_along(cv)) {
  cm = cv[[i]]  # Get confusion matrix for fold i
  accuracy = sum(diag(cm)) / sum(cm)  # Calculate accuracy
  accuracies[i] <- accuracy  # Store accuracy
}
# Print accuracies for each fold
for (i in seq_along(accuracies)) {
  cat("Accuracy for Fold", i, ":", accuracies[i], "\n")
}
# Compute the mean accuracy
mean_accuracy = mean(accuracies)
mean_accuracy #95.02%
 
```

h2o Shutdown
```{r}
#Disconnect from the h2o instance
h2o.shutdown()
```

