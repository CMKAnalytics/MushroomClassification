---
title: "Logistic Regression"
author: "Clifford Mwenda"
date: "2024-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




DATA PREPROCESSING

Import Data Set
```{r}
setwd("C:/Users/Latitude/Desktop/Mushroom Classification")
dataset = read.csv("mushroom.csv", na.strings = c("", " ", "NA"))
str(dataset)
```

Make Character Variables Factors
```{r}
library(dplyr)
dataset <- dataset %>%
  mutate(across(-c(cap.diameter, stem.height, stem.width), as.factor))
str(dataset)
```

Check DV Distribution
```{r}
prop.table(table(dataset$class))

#Here we want to confirm that the distribution between the two label data is not too much different. Because imbalanced datasets can lead to imbalanced accuracy.
```

Check NAs and Drop Columns with NAs
```{r}
n_nas = colSums(is.na(dataset))
(n_nas/61069) * 100
dataset = dataset %>% select(where(~ !any(is.na(.))))
str(dataset)
```

Re-Arrange Columns
```{r}
dataset = dataset[, c(2:12, 1)]
str(dataset)
```

Split Data-set into Training and Testing
```{r}
library("caTools")
set.seed(123)
split = sample.split(dataset$class, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

Feature Scaling
```{r}
training_set[, c(1, 6:7)] = scale(training_set[, c(1, 6:7)]) #Z Score
test_set[, c(1, 6:7)] = scale(test_set[, c(1, 6:7)])


str(training_set)
```







FITTING AND EVALUATING MODEL

Fitting Logistic Regression to the Training Set 
```{r}
classifier = glm(formula = class ~ .,
                 family = binomial,#Logistic regression with two dependent variables
                 data = training_set)
```

Predicting the test set results
```{r}
prob_pred = predict(classifier,
                    type = "response",#we get probabilities listed in a single vector if we specify response in logistic regression
                    newdata = test_set[-12])
#returns the probability the dependent variable is 1 for each of the test_set data entry


#We want predictions (0s and 1s).Not probabilities
y_pred = ifelse(prob_pred > 0.5, 1, 0)
```

Confusion Matrix and Accuracy
```{r}
#Evaluate Predictions using prediction matrix
cm = table(test_set[, 12],#vector of real values
           y_pred)#vector of predicted values
cm
accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[2, 2] + cm[1, 2] + cm[2, 1])
accuracy #72.85%
```



Apply K-Fold Cross-Validation
```{r}
library(caret)
set.seed(123)

trainctrl <- trainControl(method = "cv", number = 10, savePredictions = "all", classProbs = TRUE)#"cv" = Cross Validation #Ten_Folds

kf_fit <- train(class ~ ., data = dataset, method = "glm", family = binomial, trControl=trainctrl, tuneLength = 0)#Logistic Regression Method
#we set the tuneLength to zero because we focus on evaluating the method on each fold. We can also set the tuneLength if we want to do the parameter tuning during the cross-validation. For example, if we use the K-NN  method, and we want to analyze how many K is the best for our model.

kf_fit#73.07%

#In the output, we also received estimates of the model’s performance in terms of the average accuracy and Cohen’s kappa values across the folds (i.e., resamples). Behind the scenes, the train function has done a lot for us.The accuracy statistic represents the proportion of total correctly classified cases out of all classification instances; in the case of a logistic regression model with a dichotomous outcome variable, a correctly classified case would be one in which the actual observed outcome for the case (e.g., observed quit) matches what was predicted based on the estimated model (e.g., predicted quit). Cohen’s kappa ( κ) is another classification accuracy statistic, but it differs from the accuracy statistic in that it accounts for the baseline probabilities from your null model that contains no predictor variables. In other words, it accounts for the proportion of cases in your data that were observed to have experienced one level of the dichotomous outcome (e.g., quit) versus the other level (e.g., stay). This is useful when there isn’t a 50%/50% split in the observed levels/classes of your dichotomous outcome variable. Like so many effect size metrics, Cohen’s ( κ) has some recommended thresholds for interpreting the classification strength different  κ  values, as shown below (Landis and Koch 1977).Cohen’s  κ Description:
#.81-1.00	Almost perfect
#.61-.80	Substantial
#.41-60	Moderate
#.21-.40	Fair
#.00-.20	Slight
#< .00	Poor

# Estimate the importance of different predictors
varImp(kf_fit)
```


