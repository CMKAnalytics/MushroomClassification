---
title: "Naive Bayes"
author: "Clifford Mwenda"
date: "2024-04-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



DATA PREPROCESSING

Import Data Set
```{r}
setwd("C:/Users/Latitude/Desktop/Mushroom Classification")
dataset = read.csv("mushroom.csv", na.strings = c("", " ", "NA"))
str(dataset)
```

Make Character Variables Factors
```{r}
library(dplyr)
dataset <- dataset %>%
  mutate(across(-c(cap.diameter, stem.height, stem.width), as.factor))
str(dataset)
```

Check DV Distribution
```{r}
prop.table(table(dataset$class))

#Here we want to confirm that the distribution between the two label data is not too much different. Because imbalanced datasets can lead to imbalanced accuracy.
```

Check NAs and Drop Columns with NAs
```{r}
n_nas = colSums(is.na(dataset))
(n_nas/61069) * 100
dataset = dataset %>% select(where(~ !any(is.na(.))))
str(dataset)
```

Re-Arrange Columns
```{r}
dataset = dataset[, c(2:12, 1)]
str(dataset)
```

Convert all variables to numeric for Naive Bayes (We need a matrix of Features)
```{r}
dataset$class = as.numeric(as.integer(dataset$class))
dataset$cap.shape = as.numeric(as.integer(dataset$cap.shape))
dataset$cap.color = as.numeric(as.integer(dataset$cap.color))
dataset$does.bruise.or.bleed = as.numeric(as.integer(dataset$does.bruise.or.bleed))
dataset$gill.color = as.numeric(as.integer(dataset$gill.color))
dataset$stem.color = as.numeric(as.integer(dataset$stem.color))
dataset$has.ring = as.numeric(as.integer(dataset$has.ring))
dataset$habitat = as.numeric(as.integer(dataset$habitat))
dataset$season = as.numeric(as.integer(dataset$season))

str(dataset)
```

Split Data-set into Training and Testing
```{r}
library("caTools")
set.seed(123)
split = sample.split(dataset$class, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

Feature Scaling
```{r}
training_set[, -c(12)] = scale(training_set[, -c(12)]) #Z Score
test_set[, -c(12)] = scale(test_set[, -c(12)])


str(training_set)
```




FITTING AND EVALUATING MODEL

Fitting SVM to the Training set, Predicting the Test set results, and Making the Confusion Matrix
```{r}
library(e1071)
classifier = naiveBayes(x = training_set[-12], #Matrix of features
                        y = training_set$class)#Dependent variable


y_pred = predict(classifier, newdata = test_set[-12])


cm = table(test_set[, 12], y_pred)

accuracy = (cm[1, 1] + cm[2, 2]) / (cm[1, 1] + cm[2, 2] + cm[1, 2] + cm[2, 1])
accuracy #58.75%
```

Apply k-Fold Cross-Validation
```{r}
library(caret)

# in creating the folds we specify the target feature (dependent variable) and # of folds
folds = createFolds(training_set$class, k = 10)
# in cv we are going to applying a created function to our 'folds'
cv = lapply(folds, function(x) { # start of function
  # in the next two lines we will separate the Training set into it's 10 pieces
  training_fold = training_set[-x, ] # training fold =  training set minus (-) it's sub test fold
  test_fold = training_set[x, ] # here we describe the test fold individually
  # now apply (train) the classifer on the training_fold
  classifier = naiveBayes(x = training_set[-12], #Matrix of features
                          y = training_set$class)#Dependent variable
  # next step in the loop, we calculate the predictions and cm and we equate the accuracy
  # note we are training on training_fold and testing its accuracy on the test_fold
  y_pred = predict(classifier, newdata = test_fold[-12])
  cm = table(test_fold[, 12], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
#We compute the mean of the accuracies to get the ultimum accuracy
accuracy = mean(as.numeric(cv))
accuracy #58.57%
```

